
==> Audit <==
|---------|-----------------|----------|--------------------|---------|----------------------|----------------------|
| Command |      Args       | Profile  |        User        | Version |      Start Time      |       End Time       |
|---------|-----------------|----------|--------------------|---------|----------------------|----------------------|
| start   | --driver=docker | minikube | ONETEAM\Admin.John | v1.36.0 | 04 Jul 25 16:45 CEST |                      |
| start   | --driver=docker | minikube | ONETEAM\Admin.John | v1.36.0 | 04 Jul 25 17:24 CEST |                      |
| start   | --driver=docker | minikube | ONETEAM\Admin.John | v1.36.0 | 04 Jul 25 17:25 CEST |                      |
| start   | --driver=docker | minikube | ONETEAM\Admin.John | v1.36.0 | 04 Jul 25 17:34 CEST | 04 Jul 25 17:38 CEST |
| service | frontend --url  | minikube | ONETEAM\Admin.John | v1.36.0 | 04 Jul 25 17:41 CEST |                      |
|---------|-----------------|----------|--------------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/07/04 17:34:29
Running on machine: HK-NB-227
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0704 17:34:29.951950   21464 out.go:345] Setting OutFile to fd 96 ...
I0704 17:34:29.955303   21464 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0704 17:34:29.955303   21464 out.go:358] Setting ErrFile to fd 100...
I0704 17:34:29.955303   21464 out.go:392] TERM=,COLORTERM=, which probably does not support color
W0704 17:34:29.999663   21464 root.go:314] Error reading config file at C:\Users\Admin.John\.minikube\config\config.json: open C:\Users\Admin.John\.minikube\config\config.json: The system cannot find the file specified.
I0704 17:34:30.114144   21464 out.go:352] Setting JSON to false
I0704 17:34:30.126789   21464 start.go:130] hostinfo: {"hostname":"HK-NB-227","uptime":1116,"bootTime":1751642153,"procs":342,"os":"windows","platform":"Microsoft Windows 11 Enterprise","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.5335 Build 22631.5335","kernelVersion":"10.0.22631.5335 Build 22631.5335","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"abef010a-b26a-4840-8f6b-5ab0feec397c"}
W0704 17:34:30.126849   21464 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0704 17:34:30.130080   21464 out.go:177] * minikube v1.36.0 on Microsoft Windows 11 Enterprise 10.0.22631.5335 Build 22631.5335
I0704 17:34:30.133731   21464 notify.go:220] Checking for updates...
I0704 17:34:30.181869   21464 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
E0704 17:34:30.182873   21464 start.go:819] api.Load failed for minikube: filestore "minikube": Docker machine "minikube" does not exist. Use "docker-machine ls" to list machines. Use "docker-machine create" to add a new one.
I0704 17:34:30.184078   21464 driver.go:404] Setting default libvirt URI to qemu:///system
E0704 17:34:30.185547   21464 start.go:819] api.Load failed for minikube: filestore "minikube": Docker machine "minikube" does not exist. Use "docker-machine ls" to list machines. Use "docker-machine create" to add a new one.
I0704 17:34:30.505151   21464 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.40.0 (187762)
I0704 17:34:30.521290   21464 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0704 17:34:34.837123   21464 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (4.315832s)
I0704 17:34:34.839325   21464 info.go:266] docker info: {ID:14c3b807-df83-461d-9a68-caac78f8a289 Containers:5 ContainersRunning:2 ContainersPaused:0 ContainersStopped:3 Images:15 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:67 OomKillDisable:false NGoroutines:81 SystemTime:2025-07-04 15:34:35.079372456 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8183308288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0704 17:34:34.842117   21464 out.go:177] * Using the docker driver based on existing profile
I0704 17:34:34.844429   21464 start.go:304] selected driver: docker
I0704 17:34:34.844429   21464 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Admin.John:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0704 17:34:34.845061   21464 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0704 17:34:34.866912   21464 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0704 17:34:36.316863   21464 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.4499507s)
I0704 17:34:36.317991   21464 info.go:266] docker info: {ID:14c3b807-df83-461d-9a68-caac78f8a289 Containers:5 ContainersRunning:2 ContainersPaused:0 ContainersStopped:3 Images:15 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:67 OomKillDisable:false NGoroutines:81 SystemTime:2025-07-04 15:34:36.536935333 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8183308288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0704 17:34:36.514154   21464 cni.go:84] Creating CNI manager for ""
I0704 17:34:36.514670   21464 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0704 17:34:36.514670   21464 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Admin.John:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0704 17:34:36.518764   21464 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0704 17:34:36.522718   21464 cache.go:121] Beginning downloading kic base image for docker with docker
I0704 17:34:36.524627   21464 out.go:177] * Pulling base image v0.0.47 ...
I0704 17:34:36.528905   21464 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0704 17:34:36.529477   21464 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0704 17:34:36.530560   21464 preload.go:146] Found local preload: C:\Users\Admin.John\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0704 17:34:36.530560   21464 cache.go:56] Caching tarball of preloaded images
I0704 17:34:36.532902   21464 preload.go:172] Found C:\Users\Admin.John\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0704 17:34:36.535158   21464 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0704 17:34:36.536594   21464 profile.go:143] Saving config to C:\Users\Admin.John\.minikube\profiles\minikube\config.json ...
I0704 17:34:36.887798   21464 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0704 17:34:36.888311   21464 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0704 17:34:36.888492   21464 cache.go:230] Successfully downloaded all kic artifacts
I0704 17:34:36.889608   21464 start.go:360] acquireMachinesLock for minikube: {Name:mka07ec6c6a40d070dbfb8fce73fc83d64ee0335 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0704 17:34:36.890227   21464 start.go:364] duration metric: took 59Âµs to acquireMachinesLock for "minikube"
I0704 17:34:36.890227   21464 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Admin.John:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0704 17:34:36.890809   21464 start.go:125] createHost starting for "" (driver="docker")
I0704 17:34:36.893434   21464 out.go:235] * Creating docker container (CPUs=2, Memory=4000MB) ...
I0704 17:34:36.900145   21464 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0704 17:34:36.900145   21464 client.go:168] LocalClient.Create starting
I0704 17:34:36.902072   21464 main.go:141] libmachine: Creating CA: C:\Users\Admin.John\.minikube\certs\ca.pem
I0704 17:34:37.618977   21464 main.go:141] libmachine: Creating client certificate: C:\Users\Admin.John\.minikube\certs\cert.pem
I0704 17:34:38.521531   21464 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0704 17:34:38.980416   21464 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0704 17:34:39.001116   21464 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0704 17:34:39.001116   21464 cli_runner.go:164] Run: docker network inspect minikube
W0704 17:34:39.239675   21464 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0704 17:34:39.239675   21464 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0704 17:34:39.239675   21464 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0704 17:34:39.252122   21464 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0704 17:34:40.166521   21464 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000b22d50}
I0704 17:34:40.166521   21464 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0704 17:34:40.180424   21464 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0704 17:34:40.726437   21464 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0704 17:34:40.726437   21464 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0704 17:34:40.781939   21464 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0704 17:34:41.079829   21464 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0704 17:34:41.325204   21464 oci.go:103] Successfully created a docker volume minikube
I0704 17:34:41.344555   21464 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0704 17:34:45.966692   21464 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: (4.6221371s)
I0704 17:34:45.966692   21464 oci.go:107] Successfully prepared a docker volume minikube
I0704 17:34:45.966692   21464 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0704 17:34:45.966692   21464 kic.go:194] Starting extracting preloaded images to volume ...
I0704 17:34:45.981510   21464 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Admin.John\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
W0704 17:34:46.265017   21464 cli_runner.go:211] docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Admin.John\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir returned with exit code 125
I0704 17:34:46.265017   21464 kic.go:201] Unable to extract preloaded tarball to volume: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Admin.John\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: exit status 125
stdout:

stderr:
docker: Error response from daemon: CreateFile C:\Users\Admin.John\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4: Access is denied.

Run 'docker run --help' for more information
I0704 17:34:46.278174   21464 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0704 17:34:48.092693   21464 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.814519s)
I0704 17:34:48.094829   21464 info.go:266] docker info: {ID:14c3b807-df83-461d-9a68-caac78f8a289 Containers:5 ContainersRunning:2 ContainersPaused:0 ContainersStopped:3 Images:15 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:67 OomKillDisable:false NGoroutines:81 SystemTime:2025-07-04 15:34:48.104526745 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8183308288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0704 17:34:48.108159   21464 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0704 17:34:49.417042   21464 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.3088828s)
I0704 17:34:49.436907   21464 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0704 17:34:51.646115   21464 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b: (2.2092081s)
I0704 17:34:51.698135   21464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0704 17:34:52.196782   21464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 17:34:53.253688   21464 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.0569051s)
I0704 17:34:53.279667   21464 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0704 17:34:54.316260   21464 cli_runner.go:217] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (1.0365932s)
I0704 17:34:54.316260   21464 oci.go:144] the created container "minikube" has a running status.
I0704 17:34:54.316260   21464 kic.go:225] Creating ssh key for kic: C:\Users\Admin.John\.minikube\machines\minikube\id_rsa...
I0704 17:34:58.895904   21464 kic_runner.go:191] docker (temp): C:\Users\Admin.John\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0704 17:34:59.540745   21464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 17:35:00.181267   21464 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0704 17:35:00.181267   21464 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0704 17:35:00.758421   21464 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Admin.John\.minikube\machines\minikube\id_rsa...
I0704 17:35:07.128658   21464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 17:35:07.383172   21464 machine.go:93] provisionDockerMachine start ...
I0704 17:35:07.396299   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:07.609037   21464 main.go:141] libmachine: Using SSH client type: native
I0704 17:35:07.653641   21464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9ca9e0] 0x9cd520 <nil>  [] 0s} 127.0.0.1 50537 <nil> <nil>}
I0704 17:35:07.653641   21464 main.go:141] libmachine: About to run SSH command:
hostname
I0704 17:35:08.096905   21464 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0704 17:35:08.097417   21464 ubuntu.go:169] provisioning hostname "minikube"
I0704 17:35:08.108864   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:08.323556   21464 main.go:141] libmachine: Using SSH client type: native
I0704 17:35:08.324755   21464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9ca9e0] 0x9cd520 <nil>  [] 0s} 127.0.0.1 50537 <nil> <nil>}
I0704 17:35:08.324755   21464 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0704 17:35:08.892104   21464 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0704 17:35:08.955362   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:09.307190   21464 main.go:141] libmachine: Using SSH client type: native
I0704 17:35:09.308441   21464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9ca9e0] 0x9cd520 <nil>  [] 0s} 127.0.0.1 50537 <nil> <nil>}
I0704 17:35:09.308441   21464 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0704 17:35:09.754506   21464 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0704 17:35:09.754506   21464 ubuntu.go:175] set auth options {CertDir:C:\Users\Admin.John\.minikube CaCertPath:C:\Users\Admin.John\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Admin.John\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Admin.John\.minikube\machines\server.pem ServerKeyPath:C:\Users\Admin.John\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Admin.John\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Admin.John\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Admin.John\.minikube}
I0704 17:35:09.754506   21464 ubuntu.go:177] setting up certificates
I0704 17:35:09.754506   21464 provision.go:84] configureAuth start
I0704 17:35:09.770724   21464 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0704 17:35:10.083703   21464 provision.go:143] copyHostCerts
I0704 17:35:10.202615   21464 exec_runner.go:151] cp: C:\Users\Admin.John\.minikube\certs\key.pem --> C:\Users\Admin.John\.minikube/key.pem (1675 bytes)
I0704 17:35:10.248249   21464 exec_runner.go:151] cp: C:\Users\Admin.John\.minikube\certs\ca.pem --> C:\Users\Admin.John\.minikube/ca.pem (1086 bytes)
I0704 17:35:10.250249   21464 exec_runner.go:151] cp: C:\Users\Admin.John\.minikube\certs\cert.pem --> C:\Users\Admin.John\.minikube/cert.pem (1131 bytes)
I0704 17:35:10.252245   21464 provision.go:117] generating server cert: C:\Users\Admin.John\.minikube\machines\server.pem ca-key=C:\Users\Admin.John\.minikube\certs\ca.pem private-key=C:\Users\Admin.John\.minikube\certs\ca-key.pem org=Admin.John.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0704 17:35:10.921912   21464 provision.go:177] copyRemoteCerts
I0704 17:35:10.956032   21464 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0704 17:35:10.970021   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:11.169216   21464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50537 SSHKeyPath:C:\Users\Admin.John\.minikube\machines\minikube\id_rsa Username:docker}
I0704 17:35:11.645844   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0704 17:35:11.871563   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\machines\server.pem --> /etc/docker/server.pem (1188 bytes)
I0704 17:35:12.062722   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0704 17:35:12.250594   21464 provision.go:87] duration metric: took 2.4960878s to configureAuth
I0704 17:35:12.250594   21464 ubuntu.go:193] setting minikube options for container-runtime
I0704 17:35:12.253035   21464 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0704 17:35:12.267545   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:12.644756   21464 main.go:141] libmachine: Using SSH client type: native
I0704 17:35:12.647122   21464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9ca9e0] 0x9cd520 <nil>  [] 0s} 127.0.0.1 50537 <nil> <nil>}
I0704 17:35:12.647122   21464 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0704 17:35:13.017363   21464 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0704 17:35:13.017363   21464 ubuntu.go:71] root file system type: overlay
I0704 17:35:13.018104   21464 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0704 17:35:13.037927   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:13.368735   21464 main.go:141] libmachine: Using SSH client type: native
I0704 17:35:13.369439   21464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9ca9e0] 0x9cd520 <nil>  [] 0s} 127.0.0.1 50537 <nil> <nil>}
I0704 17:35:13.369966   21464 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0704 17:35:13.829828   21464 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0704 17:35:13.847105   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:14.111739   21464 main.go:141] libmachine: Using SSH client type: native
I0704 17:35:14.112784   21464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9ca9e0] 0x9cd520 <nil>  [] 0s} 127.0.0.1 50537 <nil> <nil>}
I0704 17:35:14.112784   21464 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0704 17:35:20.142233   21464 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-07-04 15:35:13.928807383 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0704 17:35:20.142233   21464 machine.go:96] duration metric: took 12.7585401s to provisionDockerMachine
I0704 17:35:20.142233   21464 client.go:171] duration metric: took 43.2420839s to LocalClient.Create
I0704 17:35:20.142233   21464 start.go:167] duration metric: took 43.2420839s to libmachine.API.Create "minikube"
I0704 17:35:20.147144   21464 start.go:293] postStartSetup for "minikube" (driver="docker")
I0704 17:35:20.147144   21464 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0704 17:35:20.169046   21464 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0704 17:35:20.185273   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:20.459644   21464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50537 SSHKeyPath:C:\Users\Admin.John\.minikube\machines\minikube\id_rsa Username:docker}
I0704 17:35:20.748139   21464 ssh_runner.go:195] Run: cat /etc/os-release
I0704 17:35:20.774426   21464 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0704 17:35:20.774426   21464 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0704 17:35:20.774426   21464 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0704 17:35:20.774426   21464 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0704 17:35:20.774702   21464 filesync.go:126] Scanning C:\Users\Admin.John\.minikube\addons for local assets ...
I0704 17:35:20.775213   21464 filesync.go:126] Scanning C:\Users\Admin.John\.minikube\files for local assets ...
I0704 17:35:20.775751   21464 start.go:296] duration metric: took 628.607ms for postStartSetup
I0704 17:35:20.797085   21464 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0704 17:35:21.041064   21464 profile.go:143] Saving config to C:\Users\Admin.John\.minikube\profiles\minikube\config.json ...
I0704 17:35:21.062844   21464 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0704 17:35:21.076976   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:21.386258   21464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50537 SSHKeyPath:C:\Users\Admin.John\.minikube\machines\minikube\id_rsa Username:docker}
I0704 17:35:21.668834   21464 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0704 17:35:21.700368   21464 start.go:128] duration metric: took 44.8095539s to createHost
I0704 17:35:21.700368   21464 start.go:83] releasing machines lock for "minikube", held for 44.8101358s
I0704 17:35:21.724128   21464 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0704 17:35:22.028001   21464 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0704 17:35:22.051339   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:22.067971   21464 ssh_runner.go:195] Run: cat /version.json
I0704 17:35:22.133958   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:35:22.747130   21464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50537 SSHKeyPath:C:\Users\Admin.John\.minikube\machines\minikube\id_rsa Username:docker}
I0704 17:35:22.867614   21464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50537 SSHKeyPath:C:\Users\Admin.John\.minikube\machines\minikube\id_rsa Username:docker}
I0704 17:35:23.110303   21464 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (1.0823016s)
W0704 17:35:23.110303   21464 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0704 17:35:23.189309   21464 ssh_runner.go:235] Completed: cat /version.json: (1.1213383s)
I0704 17:35:23.221557   21464 ssh_runner.go:195] Run: systemctl --version
I0704 17:35:23.307400   21464 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0704 17:35:23.369321   21464 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0704 17:35:23.496264   21464 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0704 17:35:23.559001   21464 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0704 17:35:23.880956   21464 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0704 17:35:23.880956   21464 start.go:495] detecting cgroup driver to use...
I0704 17:35:23.880956   21464 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0704 17:35:23.880956   21464 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0704 17:35:24.108944   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0704 17:35:24.242558   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0704 17:35:24.353414   21464 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0704 17:35:24.376008   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
W0704 17:35:24.415329   21464 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0704 17:35:24.476647   21464 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0704 17:35:24.492737   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0704 17:35:24.599242   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0704 17:35:24.678271   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0704 17:35:24.763831   21464 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0704 17:35:24.858344   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0704 17:35:24.985738   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0704 17:35:25.106170   21464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0704 17:35:25.190556   21464 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0704 17:35:25.274829   21464 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0704 17:35:25.376058   21464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 17:35:25.901798   21464 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0704 17:35:26.469641   21464 start.go:495] detecting cgroup driver to use...
I0704 17:35:26.469641   21464 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0704 17:35:26.491236   21464 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0704 17:35:26.575349   21464 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0704 17:35:26.595960   21464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0704 17:35:26.684627   21464 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0704 17:35:27.001115   21464 ssh_runner.go:195] Run: which cri-dockerd
I0704 17:35:27.093109   21464 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0704 17:35:27.177256   21464 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0704 17:35:27.470069   21464 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0704 17:35:28.183833   21464 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0704 17:35:28.640169   21464 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0704 17:35:28.640740   21464 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0704 17:35:28.776437   21464 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0704 17:35:28.891332   21464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 17:35:29.573376   21464 ssh_runner.go:195] Run: sudo systemctl restart docker
I0704 17:35:33.373740   21464 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.7998405s)
I0704 17:35:33.399575   21464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0704 17:35:33.498310   21464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0704 17:35:33.612459   21464 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0704 17:35:34.156309   21464 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0704 17:35:34.564100   21464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 17:35:34.917350   21464 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0704 17:35:35.053236   21464 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0704 17:35:35.122377   21464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 17:35:35.580057   21464 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0704 17:35:36.825830   21464 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (1.2457733s)
I0704 17:35:36.862542   21464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0704 17:35:36.957459   21464 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0704 17:35:36.978030   21464 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0704 17:35:37.019794   21464 start.go:563] Will wait 60s for crictl version
I0704 17:35:37.044861   21464 ssh_runner.go:195] Run: which crictl
I0704 17:35:37.094802   21464 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0704 17:35:37.768420   21464 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0704 17:35:37.784533   21464 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0704 17:35:38.393737   21464 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0704 17:35:38.771442   21464 out.go:235] * Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0704 17:35:38.793617   21464 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0704 17:35:39.859484   21464 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (1.0653412s)
I0704 17:35:39.859484   21464 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0704 17:35:39.877980   21464 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0704 17:35:39.914482   21464 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0704 17:35:40.003492   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0704 17:35:40.240504   21464 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Admin.John:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0704 17:35:40.240504   21464 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0704 17:35:40.255997   21464 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0704 17:35:40.484709   21464 docker.go:702] Got preloaded images: 
I0704 17:35:40.484709   21464 docker.go:708] registry.k8s.io/kube-apiserver:v1.33.1 wasn't preloaded
I0704 17:35:40.501225   21464 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0704 17:35:40.570905   21464 ssh_runner.go:195] Run: which lz4
I0704 17:35:40.657329   21464 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I0704 17:35:40.677704   21464 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0704 17:35:40.677704   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (363893849 bytes)
I0704 17:36:16.180518   21464 docker.go:666] duration metric: took 35.5402195s to copy over tarball
I0704 17:36:16.214072   21464 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I0704 17:36:40.573501   21464 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (24.3594268s)
I0704 17:36:40.573501   21464 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0704 17:36:41.071279   21464 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0704 17:36:41.150903   21464 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I0704 17:36:41.310862   21464 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0704 17:36:41.398815   21464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 17:36:41.863942   21464 ssh_runner.go:195] Run: sudo systemctl restart docker
I0704 17:36:45.796430   21464 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.9319624s)
I0704 17:36:45.840195   21464 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0704 17:36:46.006332   21464 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0704 17:36:46.006332   21464 cache_images.go:84] Images are preloaded, skipping loading
I0704 17:36:46.006933   21464 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0704 17:36:46.028005   21464 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0704 17:36:46.055275   21464 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0704 17:36:47.145890   21464 cni.go:84] Creating CNI manager for ""
I0704 17:36:47.145890   21464 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0704 17:36:47.199837   21464 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0704 17:36:47.199837   21464 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0704 17:36:47.200348   21464 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0704 17:36:47.241606   21464 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0704 17:36:47.401488   21464 binaries.go:44] Found k8s binaries, skipping transfer
I0704 17:36:47.437676   21464 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0704 17:36:47.555572   21464 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0704 17:36:47.731248   21464 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0704 17:36:47.902191   21464 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0704 17:36:48.109060   21464 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0704 17:36:48.149900   21464 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0704 17:36:48.375515   21464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 17:36:49.147438   21464 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0704 17:36:49.332173   21464 certs.go:68] Setting up C:\Users\Admin.John\.minikube\profiles\minikube for IP: 192.168.49.2
I0704 17:36:49.332243   21464 certs.go:194] generating shared ca certs ...
I0704 17:36:49.332966   21464 certs.go:226] acquiring lock for ca certs: {Name:mkb575fc520fc79bc486b2bba8eb9c2064c3829b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:49.335610   21464 certs.go:240] generating "minikubeCA" ca cert: C:\Users\Admin.John\.minikube\ca.key
I0704 17:36:49.858355   21464 crypto.go:156] Writing cert to C:\Users\Admin.John\.minikube\ca.crt ...
I0704 17:36:49.858355   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\ca.crt: {Name:mk6d7e97da1c96369267ec363442f2c2e1d1fdf0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:49.877898   21464 crypto.go:164] Writing key to C:\Users\Admin.John\.minikube\ca.key ...
I0704 17:36:49.877898   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\ca.key: {Name:mk4c28db058978d15921131813f4896228ecdb0d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:49.880901   21464 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\Admin.John\.minikube\proxy-client-ca.key
I0704 17:36:52.072714   21464 crypto.go:156] Writing cert to C:\Users\Admin.John\.minikube\proxy-client-ca.crt ...
I0704 17:36:52.072714   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\proxy-client-ca.crt: {Name:mk96743f69195e997fb38454a526e975aadcde7e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:52.082118   21464 crypto.go:164] Writing key to C:\Users\Admin.John\.minikube\proxy-client-ca.key ...
I0704 17:36:52.082118   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\proxy-client-ca.key: {Name:mk735bd84e222d517e38083fa54f46cc5b4712bc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:52.118126   21464 certs.go:256] generating profile certs ...
I0704 17:36:52.120167   21464 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Admin.John\.minikube\profiles\minikube\client.key
I0704 17:36:52.146093   21464 crypto.go:68] Generating cert C:\Users\Admin.John\.minikube\profiles\minikube\client.crt with IP's: []
I0704 17:36:53.557239   21464 crypto.go:156] Writing cert to C:\Users\Admin.John\.minikube\profiles\minikube\client.crt ...
I0704 17:36:53.557239   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\profiles\minikube\client.crt: {Name:mk24f1d739f16521fa303d60b14d4226e3f50343 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:53.581082   21464 crypto.go:164] Writing key to C:\Users\Admin.John\.minikube\profiles\minikube\client.key ...
I0704 17:36:53.581082   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\profiles\minikube\client.key: {Name:mk2b35a83a0fbfc86433388c120fcc5cb83bb849 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:53.585589   21464 certs.go:363] generating signed profile cert for "minikube": C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0704 17:36:53.585589   21464 crypto.go:68] Generating cert C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0704 17:36:56.332771   21464 crypto.go:156] Writing cert to C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0704 17:36:56.332771   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk1b8dfa8f1c4f52a238ae30199f7e2491369f08 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:56.336188   21464 crypto.go:164] Writing key to C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0704 17:36:56.336188   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk4e99d0010705d322817879a246dd7832b01c29 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:56.371751   21464 certs.go:381] copying C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.crt
I0704 17:36:56.406215   21464 certs.go:385] copying C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.key
I0704 17:36:56.421610   21464 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.key
I0704 17:36:56.422128   21464 crypto.go:68] Generating cert C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0704 17:36:57.036651   21464 crypto.go:156] Writing cert to C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.crt ...
I0704 17:36:57.036651   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.crt: {Name:mk1c705ced577bf05c032ce77ac1a24d1025e307 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:57.042327   21464 crypto.go:164] Writing key to C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.key ...
I0704 17:36:57.042327   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.key: {Name:mk56770f396e0e1326fc23cd7c996fa635c5db98 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:36:57.106271   21464 certs.go:484] found cert: C:\Users\Admin.John\.minikube\certs\ca-key.pem (1679 bytes)
I0704 17:36:57.149142   21464 certs.go:484] found cert: C:\Users\Admin.John\.minikube\certs\ca.pem (1086 bytes)
I0704 17:36:57.184592   21464 certs.go:484] found cert: C:\Users\Admin.John\.minikube\certs\cert.pem (1131 bytes)
I0704 17:36:57.226389   21464 certs.go:484] found cert: C:\Users\Admin.John\.minikube\certs\key.pem (1675 bytes)
I0704 17:36:57.292194   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0704 17:36:57.518924   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0704 17:36:57.739538   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0704 17:36:57.947236   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0704 17:36:58.167628   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0704 17:36:58.369247   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0704 17:36:58.593693   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0704 17:36:58.807657   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0704 17:36:58.978364   21464 ssh_runner.go:362] scp C:\Users\Admin.John\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0704 17:36:59.183195   21464 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0704 17:36:59.361544   21464 ssh_runner.go:195] Run: openssl version
I0704 17:36:59.439774   21464 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0704 17:36:59.539992   21464 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0704 17:36:59.570480   21464 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul  4 15:36 /usr/share/ca-certificates/minikubeCA.pem
I0704 17:36:59.586850   21464 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0704 17:36:59.669790   21464 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0704 17:36:59.765342   21464 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0704 17:36:59.795408   21464 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0704 17:36:59.796049   21464 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Admin.John:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0704 17:36:59.810844   21464 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0704 17:36:59.978021   21464 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0704 17:37:00.074262   21464 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0704 17:37:00.153214   21464 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0704 17:37:00.170083   21464 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0704 17:37:00.251922   21464 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0704 17:37:00.251922   21464 kubeadm.go:157] found existing configuration files:

I0704 17:37:00.270507   21464 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0704 17:37:00.394848   21464 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0704 17:37:00.412655   21464 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0704 17:37:00.513546   21464 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0704 17:37:00.593156   21464 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0704 17:37:00.610286   21464 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0704 17:37:00.722785   21464 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0704 17:37:00.818333   21464 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0704 17:37:00.836977   21464 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0704 17:37:00.945735   21464 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0704 17:37:01.039658   21464 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0704 17:37:01.063312   21464 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0704 17:37:01.146690   21464 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0704 17:37:01.593020   21464 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0704 17:37:01.593020   21464 kubeadm.go:310] [preflight] Running pre-flight checks
I0704 17:37:02.490642   21464 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0704 17:37:02.492224   21464 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0704 17:37:02.492224   21464 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0704 17:37:02.646441   21464 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0704 17:37:02.650872   21464 out.go:235]   - Generating certificates and keys ...
I0704 17:37:02.669393   21464 kubeadm.go:310] [certs] Using existing ca certificate authority
I0704 17:37:02.669393   21464 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0704 17:37:04.719862   21464 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0704 17:37:06.014014   21464 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0704 17:37:15.726212   21464 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0704 17:37:17.976826   21464 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0704 17:37:18.662273   21464 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0704 17:37:18.662770   21464 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0704 17:37:19.339400   21464 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0704 17:37:19.340102   21464 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0704 17:37:19.703834   21464 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0704 17:37:20.403607   21464 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0704 17:37:21.421770   21464 kubeadm.go:310] [certs] Generating "sa" key and public key
I0704 17:37:21.428999   21464 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0704 17:37:25.067609   21464 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0704 17:37:26.784706   21464 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0704 17:37:29.144246   21464 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0704 17:37:30.286153   21464 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0704 17:37:31.003794   21464 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0704 17:37:31.005455   21464 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0704 17:37:31.013048   21464 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0704 17:37:31.017072   21464 out.go:235]   - Booting up control plane ...
I0704 17:37:31.019615   21464 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0704 17:37:31.019729   21464 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0704 17:37:31.019729   21464 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0704 17:37:31.050299   21464 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0704 17:37:31.070146   21464 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0704 17:37:31.070146   21464 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0704 17:37:31.427774   21464 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0704 17:37:31.428286   21464 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0704 17:37:33.463616   21464 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 2.004056338s
I0704 17:37:33.474124   21464 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0704 17:37:33.474758   21464 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0704 17:37:33.475271   21464 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0704 17:37:33.475966   21464 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0704 17:37:50.751490   21464 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 16.997092553s
I0704 17:37:55.744892   21464 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 21.906697182s
I0704 17:38:00.424072   21464 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 26.503766851s
I0704 17:38:00.535491   21464 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0704 17:38:00.644693   21464 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0704 17:38:00.783730   21464 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0704 17:38:00.784373   21464 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0704 17:38:00.825885   21464 kubeadm.go:310] [bootstrap-token] Using token: 53pq2a.91g3ujsb9vea6prm
I0704 17:38:00.827739   21464 out.go:235]   - Configuring RBAC rules ...
I0704 17:38:00.833828   21464 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0704 17:38:00.856646   21464 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0704 17:38:00.899576   21464 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0704 17:38:00.931631   21464 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0704 17:38:00.949420   21464 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0704 17:38:00.963069   21464 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0704 17:38:01.039561   21464 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0704 17:38:03.494565   21464 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0704 17:38:03.957061   21464 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0704 17:38:03.957663   21464 kubeadm.go:310] 
I0704 17:38:03.958234   21464 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0704 17:38:03.958234   21464 kubeadm.go:310] 
I0704 17:38:03.958234   21464 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0704 17:38:03.958234   21464 kubeadm.go:310] 
I0704 17:38:03.958746   21464 kubeadm.go:310]   mkdir -p $HOME/.kube
I0704 17:38:03.958746   21464 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0704 17:38:03.959304   21464 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0704 17:38:03.959304   21464 kubeadm.go:310] 
I0704 17:38:03.959304   21464 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0704 17:38:03.959304   21464 kubeadm.go:310] 
I0704 17:38:03.959866   21464 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0704 17:38:03.959866   21464 kubeadm.go:310] 
I0704 17:38:03.960421   21464 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0704 17:38:03.960985   21464 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0704 17:38:03.960985   21464 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0704 17:38:03.960985   21464 kubeadm.go:310] 
I0704 17:38:03.961534   21464 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0704 17:38:03.962553   21464 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0704 17:38:03.962553   21464 kubeadm.go:310] 
I0704 17:38:03.962553   21464 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 53pq2a.91g3ujsb9vea6prm \
I0704 17:38:03.963553   21464 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:d24a4b13442aa3e155f937f288c891ee1f7ca4c3990264c8b88bd569b904be9d \
I0704 17:38:03.963553   21464 kubeadm.go:310] 	--control-plane 
I0704 17:38:03.963553   21464 kubeadm.go:310] 
I0704 17:38:03.964066   21464 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0704 17:38:03.964066   21464 kubeadm.go:310] 
I0704 17:38:03.964066   21464 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 53pq2a.91g3ujsb9vea6prm \
I0704 17:38:03.965079   21464 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:d24a4b13442aa3e155f937f288c891ee1f7ca4c3990264c8b88bd569b904be9d 
I0704 17:38:03.985512   21464 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0704 17:38:03.986673   21464 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0704 17:38:03.986673   21464 cni.go:84] Creating CNI manager for ""
I0704 17:38:03.987186   21464 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0704 17:38:03.989230   21464 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0704 17:38:04.014800   21464 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0704 17:38:04.282558   21464 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0704 17:38:04.977726   21464 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0704 17:38:05.011438   21464 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_07_04T17_38_04_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0704 17:38:05.014060   21464 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0704 17:38:06.720947   21464 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (1.7432211s)
I0704 17:38:06.720947   21464 ops.go:34] apiserver oom_adj: -16
I0704 17:38:10.369082   21464 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (5.3540199s)
I0704 17:38:10.369082   21464 kubeadm.go:1105] duration metric: took 5.3860996s to wait for elevateKubeSystemPrivileges
I0704 17:38:10.777633   21464 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_07_04T17_38_04_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (5.7661946s)
I0704 17:38:10.777633   21464 kubeadm.go:394] duration metric: took 1m10.9815771s to StartCluster
I0704 17:38:10.777633   21464 settings.go:142] acquiring lock: {Name:mk457201c2ac2948374883a3d471c09be491410d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:38:10.777633   21464 settings.go:150] Updating kubeconfig:  C:\Users\Admin.John\.kube\config
I0704 17:38:10.784108   21464 lock.go:35] WriteFile acquiring C:\Users\Admin.John\.kube\config: {Name:mkb38419c90074bddc68cfac4106a638e6d74ba2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 17:38:10.800791   21464 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0704 17:38:10.803873   21464 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0704 17:38:10.807852   21464 out.go:177] * Verifying Kubernetes components...
I0704 17:38:10.812285   21464 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0704 17:38:10.815917   21464 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0704 17:38:10.815974   21464 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0704 17:38:10.817164   21464 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0704 17:38:10.820421   21464 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0704 17:38:10.821509   21464 host.go:66] Checking if "minikube" exists ...
I0704 17:38:10.834760   21464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 17:38:10.896248   21464 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0704 17:38:10.932272   21464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 17:38:10.932272   21464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 17:38:11.360096   21464 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0704 17:38:11.368963   21464 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0704 17:38:11.368963   21464 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0704 17:38:11.388485   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:38:11.409638   21464 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0704 17:38:11.410416   21464 host.go:66] Checking if "minikube" exists ...
I0704 17:38:11.442301   21464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 17:38:11.716250   21464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50537 SSHKeyPath:C:\Users\Admin.John\.minikube\machines\minikube\id_rsa Username:docker}
I0704 17:38:11.783578   21464 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0704 17:38:11.783578   21464 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0704 17:38:11.814147   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 17:38:12.151333   21464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50537 SSHKeyPath:C:\Users\Admin.John\.minikube\machines\minikube\id_rsa Username:docker}
I0704 17:38:13.004186   21464 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (2.1694255s)
I0704 17:38:13.012259   21464 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (2.2083854s)
I0704 17:38:13.012259   21464 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0704 17:38:13.024308   21464 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0704 17:38:13.739400   21464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0704 17:38:13.848653   21464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 17:38:13.982988   21464 api_server.go:52] waiting for apiserver process to appear ...
I0704 17:38:13.999450   21464 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 17:38:14.651074   21464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0704 17:38:19.218537   21464 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (6.206277s)
I0704 17:38:19.219489   21464 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0704 17:38:19.833191   21464 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0704 17:38:22.261753   21464 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (8.2623022s)
I0704 17:38:22.261753   21464 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (8.4130996s)
I0704 17:38:22.261753   21464 api_server.go:72] duration metric: took 11.4604467s to wait for apiserver process to appear ...
I0704 17:38:22.261753   21464 api_server.go:88] waiting for apiserver healthz status ...
I0704 17:38:22.261753   21464 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (7.6106787s)
I0704 17:38:22.263402   21464 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50536/healthz ...
I0704 17:38:22.501891   21464 api_server.go:279] https://127.0.0.1:50536/healthz returned 200:
ok
I0704 17:38:22.719355   21464 api_server.go:141] control plane version: v1.33.1
I0704 17:38:22.719355   21464 api_server.go:131] duration metric: took 457.6023ms to wait for apiserver health ...
I0704 17:38:22.795019   21464 system_pods.go:43] waiting for kube-system pods to appear ...
I0704 17:38:22.872963   21464 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0704 17:38:22.886515   21464 addons.go:514] duration metric: took 12.0845741s for enable addons: enabled=[storage-provisioner default-storageclass]
I0704 17:38:23.238971   21464 system_pods.go:59] 8 kube-system pods found
I0704 17:38:23.238971   21464 system_pods.go:61] "coredns-674b8bbfcf-455fm" [2942e51c-ed3c-481a-92fd-473ba3c90d6a] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0704 17:38:23.238971   21464 system_pods.go:61] "coredns-674b8bbfcf-mkjfh" [b326e762-29f6-4b56-98a2-f4aa55802cba] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0704 17:38:23.238971   21464 system_pods.go:61] "etcd-minikube" [7cc017c8-ca80-4cbc-bf13-1e4cbfe78b69] Running
I0704 17:38:23.239478   21464 system_pods.go:61] "kube-apiserver-minikube" [70b0859e-07ef-4f7f-a4ec-56274fb58c58] Running
I0704 17:38:23.239996   21464 system_pods.go:61] "kube-controller-manager-minikube" [55218601-ef2a-47a1-9590-099bc1346a4d] Running
I0704 17:38:23.239996   21464 system_pods.go:61] "kube-proxy-rwwx5" [a6af43dc-b2cc-48ff-9efe-84766a7d51df] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0704 17:38:23.239996   21464 system_pods.go:61] "kube-scheduler-minikube" [bc258875-5bdf-42cc-ba6d-c6a55dfc4f81] Running
I0704 17:38:23.240061   21464 system_pods.go:61] "storage-provisioner" [b1d5c6a5-dd2c-41e0-b6e4-84f23da3d79d] Pending / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0704 17:38:23.240061   21464 system_pods.go:74] duration metric: took 445.0417ms to wait for pod list to return data ...
I0704 17:38:23.240061   21464 kubeadm.go:578] duration metric: took 12.4387545s to wait for: map[apiserver:true system_pods:true]
I0704 17:38:23.240061   21464 node_conditions.go:102] verifying NodePressure condition ...
I0704 17:38:23.350321   21464 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0704 17:38:23.350321   21464 node_conditions.go:123] node cpu capacity is 8
I0704 17:38:23.354486   21464 node_conditions.go:105] duration metric: took 113.3355ms to run NodePressure ...
I0704 17:38:23.354486   21464 start.go:241] waiting for startup goroutines ...
I0704 17:38:23.354486   21464 start.go:246] waiting for cluster config update ...
I0704 17:38:23.354486   21464 start.go:255] writing updated cluster config ...
I0704 17:38:23.635199   21464 ssh_runner.go:195] Run: rm -f paused
I0704 17:38:31.351897   21464 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0704 17:38:31.355359   21464 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 04 15:36:42 minikube systemd[1]: Stopping Docker Application Container Engine...
Jul 04 15:36:42 minikube dockerd[1332]: time="2025-07-04T15:36:42.104340159Z" level=info msg="Processing signal 'terminated'"
Jul 04 15:36:42 minikube dockerd[1332]: time="2025-07-04T15:36:42.108768617Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jul 04 15:36:42 minikube dockerd[1332]: time="2025-07-04T15:36:42.110564424Z" level=info msg="Daemon shutdown complete"
Jul 04 15:36:42 minikube dockerd[1332]: time="2025-07-04T15:36:42.110783778Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Jul 04 15:36:42 minikube systemd[1]: docker.service: Deactivated successfully.
Jul 04 15:36:42 minikube systemd[1]: Stopped Docker Application Container Engine.
Jul 04 15:36:42 minikube systemd[1]: docker.service: Consumed 2.516s CPU time.
Jul 04 15:36:42 minikube systemd[1]: Starting Docker Application Container Engine...
Jul 04 15:36:42 minikube dockerd[1864]: time="2025-07-04T15:36:42.409682506Z" level=info msg="Starting up"
Jul 04 15:36:42 minikube dockerd[1864]: time="2025-07-04T15:36:42.423976327Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jul 04 15:36:42 minikube dockerd[1864]: time="2025-07-04T15:36:42.523499648Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jul 04 15:36:42 minikube dockerd[1864]: time="2025-07-04T15:36:42.584347237Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jul 04 15:36:43 minikube dockerd[1864]: time="2025-07-04T15:36:43.128842736Z" level=info msg="Loading containers: start."
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.411842192Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count a0a40394597c5e6d3fb232263737b3d9920a6fe5234fc01fd6d7beb8bd4a99d6], retrying...."
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.639684251Z" level=info msg="Loading containers: done."
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.720466551Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.720664107Z" level=info msg="Initializing buildkit"
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.875656419Z" level=info msg="Completed buildkit initialization"
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.917741763Z" level=info msg="Daemon has completed initialization"
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.918273746Z" level=info msg="API listen on [::]:2376"
Jul 04 15:36:45 minikube dockerd[1864]: time="2025-07-04T15:36:45.918308466Z" level=info msg="API listen on /var/run/docker.sock"
Jul 04 15:36:45 minikube systemd[1]: Started Docker Application Container Engine.
Jul 04 15:37:36 minikube cri-dockerd[1640]: time="2025-07-04T15:37:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d251f05d0e454e52f8683a1dac2328e288ea491a9bbe4c63e1d73057a2611d94/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:37:36 minikube cri-dockerd[1640]: time="2025-07-04T15:37:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a24e401374160c9182d178f5545a4f7ca800ddbd1e3c497a2e2dbcfaff3e6b4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:37:36 minikube cri-dockerd[1640]: time="2025-07-04T15:37:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/69996718879b15499dc034a4d51fdfe3200aca4d60ea1f81d30375153f9058f0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:37:36 minikube cri-dockerd[1640]: time="2025-07-04T15:37:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c84cff14968b4161cc4b8c210099916db4787fffeb0d05d19288d4af9d8dda0c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:38:20 minikube cri-dockerd[1640]: time="2025-07-04T15:38:20Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jul 04 15:38:21 minikube cri-dockerd[1640]: time="2025-07-04T15:38:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a4f73edebc4de5980419a26571fdc1904a823a480f8fc275b07707fd60d762ef/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:38:21 minikube cri-dockerd[1640]: time="2025-07-04T15:38:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5771faab523b74267d525ca3306d119d848f3e65ccbc79fde81ee9c9bd7638a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:38:21 minikube cri-dockerd[1640]: time="2025-07-04T15:38:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9817a2dc5a73a5a9113d910b9e42740f4d4fee35b57d65d4ba8db21df85ead49/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:38:26 minikube cri-dockerd[1640]: time="2025-07-04T15:38:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/418e6ff969dc9eafe4abafed8c910ca3369ce9382785d7db576b3361d7228c09/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jul 04 15:38:36 minikube dockerd[1864]: time="2025-07-04T15:38:36.948773757Z" level=info msg="ignoring event" container=659c64cfb572aaf73c745fdb27d4814c2f862e5739f5954969ffde5c8843cffd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:38:37 minikube cri-dockerd[1640]: time="2025-07-04T15:38:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-455fm_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Jul 04 15:38:37 minikube dockerd[1864]: time="2025-07-04T15:38:37.531423792Z" level=info msg="ignoring event" container=9817a2dc5a73a5a9113d910b9e42740f4d4fee35b57d65d4ba8db21df85ead49 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:40:50 minikube cri-dockerd[1640]: time="2025-07-04T15:40:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0413ce7915a8ef249553b8e6b8f2a4b41ffe092e4c621d1bbeb0bc3fa45c49ce/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:40:50 minikube cri-dockerd[1640]: time="2025-07-04T15:40:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6f88681581f9964d8fe9b522e8c129f1d94c2afded0dd92e92cfeac9d493ffba/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:40:52 minikube cri-dockerd[1640]: time="2025-07-04T15:40:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ca8ed524a25bb29772754cb689db2c6103a9f2b4b031036fed4a7c45f682a7c2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:40:53 minikube cri-dockerd[1640]: time="2025-07-04T15:40:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3dd7dcdedcd6091cf10763ef790348f4e4cddf305520a0a3565e487979ff8059/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:40:53 minikube cri-dockerd[1640]: time="2025-07-04T15:40:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/52354269b29636a686d86e6d0145d4f861caa207cd90a86a07fd41d227e2d9e1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:40:53 minikube cri-dockerd[1640]: time="2025-07-04T15:40:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6889b372968df84236779540f70f428d7c153454c504be10bec19cd2e1ffd3ea/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:40:56 minikube dockerd[1864]: time="2025-07-04T15:40:56.559207370Z" level=info msg="ignoring event" container=6889b372968df84236779540f70f428d7c153454c504be10bec19cd2e1ffd3ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:40:57 minikube cri-dockerd[1640]: time="2025-07-04T15:40:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7ee182fdc5fc992060322f76681731dd13a02ebe64b0254fa7303ae12e3b3701/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:41:00 minikube dockerd[1864]: time="2025-07-04T15:41:00.155764856Z" level=info msg="ignoring event" container=7ee182fdc5fc992060322f76681731dd13a02ebe64b0254fa7303ae12e3b3701 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:41:03 minikube cri-dockerd[1640]: time="2025-07-04T15:41:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0a58b498a754b24247fcd1863a487c10e036c788073c8d47584e4b981b65179a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:41:07 minikube cri-dockerd[1640]: time="2025-07-04T15:41:07Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Downloading [=>                                                 ]  3.222MB/112.8MB"
Jul 04 15:41:17 minikube cri-dockerd[1640]: time="2025-07-04T15:41:17Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Downloading [========================>                          ]  55.96MB/112.8MB"
Jul 04 15:41:27 minikube cri-dockerd[1640]: time="2025-07-04T15:41:27Z" level=info msg="Pulling image postgres:latest: 3da95a905ed5: Extracting [============================================>      ]  25.36MB/28.23MB"
Jul 04 15:41:37 minikube cri-dockerd[1640]: time="2025-07-04T15:41:37Z" level=info msg="Pulling image postgres:latest: e9068395fab4: Extracting [===========================>                       ]  4.424MB/8.066MB"
Jul 04 15:41:47 minikube cri-dockerd[1640]: time="2025-07-04T15:41:47Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Extracting [======>                                            ]  15.04MB/112.8MB"
Jul 04 15:41:57 minikube cri-dockerd[1640]: time="2025-07-04T15:41:57Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Extracting [==========================>                        ]  59.05MB/112.8MB"
Jul 04 15:42:08 minikube cri-dockerd[1640]: time="2025-07-04T15:42:08Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Extracting [===============================================>   ]  108.1MB/112.8MB"
Jul 04 15:42:13 minikube cri-dockerd[1640]: time="2025-07-04T15:42:13Z" level=info msg="Stop pulling image postgres:latest: Status: Downloaded newer image for postgres:latest"
Jul 04 15:42:26 minikube cri-dockerd[1640]: time="2025-07-04T15:42:26Z" level=info msg="Pulling image mongo:latest: b08e2ff4391e: Downloading [===============>                                   ]  9.268MB/29.72MB"
Jul 04 15:42:36 minikube cri-dockerd[1640]: time="2025-07-04T15:42:36Z" level=info msg="Pulling image mongo:latest: b08e2ff4391e: Downloading [========================================>          ]   24.1MB/29.72MB"
Jul 04 15:42:47 minikube cri-dockerd[1640]: time="2025-07-04T15:42:47Z" level=info msg="Pulling image mongo:latest: b9e8e2e5bd69: Downloading [=====>                                             ]  29.58MB/262.2MB"
Jul 04 15:42:57 minikube cri-dockerd[1640]: time="2025-07-04T15:42:57Z" level=info msg="Pulling image mongo:latest: b08e2ff4391e: Extracting [=================================>                 ]  19.66MB/29.72MB"
Jul 04 15:43:07 minikube cri-dockerd[1640]: time="2025-07-04T15:43:07Z" level=info msg="Pulling image mongo:latest: b08e2ff4391e: Extracting [==============================================>    ]  27.85MB/29.72MB"
Jul 04 15:43:17 minikube cri-dockerd[1640]: time="2025-07-04T15:43:17Z" level=info msg="Pulling image mongo:latest: b9e8e2e5bd69: Downloading [======================>                            ]  120.5MB/262.2MB"
Jul 04 15:43:27 minikube cri-dockerd[1640]: time="2025-07-04T15:43:27Z" level=info msg="Pulling image mongo:latest: c7b0a7e2c789: Pull complete "


==> container status <==
CONTAINER           IMAGE                                                                              CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
27aa2ea19d4c2       postgres@sha256:3962158596daaef3682838cc8eb0e719ad1ce520f88e34596ce8d5de1b6330a1   About a minute ago   Running             postgres                  0                   3dd7dcdedcd60       postgres-ff9f59647-5kwjj
ac44aead8e6da       6e38f40d628db                                                                      5 minutes ago        Running             storage-provisioner       0                   418e6ff969dc9       storage-provisioner
0745e7623abca       1cf5f116067c6                                                                      5 minutes ago        Running             coredns                   0                   e5771faab523b       coredns-674b8bbfcf-mkjfh
5c1db6a0cd4ae       b79c189b052cd                                                                      5 minutes ago        Running             kube-proxy                0                   a4f73edebc4de       kube-proxy-rwwx5
1c3e862694787       499038711c081                                                                      5 minutes ago        Running             etcd                      0                   69996718879b1       etcd-minikube
a00d2ca4abe8c       c6ab243b29f82                                                                      5 minutes ago        Running             kube-apiserver            0                   c84cff14968b4       kube-apiserver-minikube
b4ad6bf0b1710       398c985c0d950                                                                      5 minutes ago        Running             kube-scheduler            0                   5a24e40137416       kube-scheduler-minikube
cda789bec2014       ef43894fa110c                                                                      5 minutes ago        Running             kube-controller-manager   0                   d251f05d0e454       kube-controller-manager-minikube


==> coredns [0745e7623abc] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] Reloading
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
[INFO] Reloading complete
[INFO] 127.0.0.1:50013 - 37460 "HINFO IN 1631186213523686496.16297435968454227. udp 55 false 512" NXDOMAIN qr,rd,ra 55 1.314809307s
[INFO] 127.0.0.1:35575 - 12496 "HINFO IN 1631186213523686496.16297435968454227. udp 55 false 512" NXDOMAIN qr,rd,ra 55 4.485775334s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_04T17_38_04_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 04 Jul 2025 15:37:56 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 04 Jul 2025 15:43:29 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 04 Jul 2025 15:42:41 +0000   Fri, 04 Jul 2025 15:37:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 04 Jul 2025 15:42:41 +0000   Fri, 04 Jul 2025 15:37:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 04 Jul 2025 15:42:41 +0000   Fri, 04 Jul 2025 15:37:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 04 Jul 2025 15:42:41 +0000   Fri, 04 Jul 2025 15:37:59 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7991512Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7991512Ki
  pods:               110
System Info:
  Machine ID:                 ee49dbbb05ab4f4faf1f890e6dd95877
  System UUID:                ee49dbbb05ab4f4faf1f890e6dd95877
  Boot ID:                    461c586f-db7d-43f5-9262-a8b631e36873
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     auth-service-6cf6d597f4-hck22        0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m46s
  default                     frontend-57f7fbf4-24n2c              0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m46s
  default                     mongo-db-54cb6f854f-c8jr5            0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m45s
  default                     monitoring-service-9d4457dc-bnz96    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m45s
  default                     postgres-ff9f59647-5kwjj             0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m45s
  default                     travel-service-697db68659-n5t4r      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m43s
  kube-system                 coredns-674b8bbfcf-mkjfh             100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     5m18s
  kube-system                 etcd-minikube                        100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         5m37s
  kube-system                 kube-apiserver-minikube              250m (3%)     0 (0%)      0 (0%)           0 (0%)         5m37s
  kube-system                 kube-controller-manager-minikube     200m (2%)     0 (0%)      0 (0%)           0 (0%)         5m37s
  kube-system                 kube-proxy-rwwx5                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m19s
  kube-system                 kube-scheduler-minikube              100m (1%)     0 (0%)      0 (0%)           0 (0%)         5m37s
  kube-system                 storage-provisioner                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m11s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age              From             Message
  ----    ------                   ----             ----             -------
  Normal  Starting                 5m6s             kube-proxy       
  Normal  NodeHasSufficientMemory  6m (x8 over 6m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    6m (x8 over 6m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     6m (x7 over 6m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  6m               kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 5m29s            kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  5m26s            kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  5m25s            kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m25s            kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m25s            kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           5m20s            node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.437224] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[ +23.372806] WSL (218) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +2.926170] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2119: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.033185] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.809839] pulseaudio[254]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +1.582369] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.067658] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.096646] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.048876] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.069112] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Jul 4 15:31] /dev/sdf: Can't open blockdev
[  +7.843605] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.041785] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.018779] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.039690] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.012066] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.505788] Failed to connect to bus: No such file or directory
[  +0.312817] Failed to connect to bus: No such file or directory
[  +0.323496] Failed to connect to bus: No such file or directory
[  +0.400184] Failed to connect to bus: No such file or directory
[  +0.365171] Failed to connect to bus: No such file or directory
[  +0.360663] Failed to connect to bus: No such file or directory
[  +0.523151] Failed to connect to bus: No such file or directory
[  +0.315460] Failed to connect to bus: No such file or directory
[  +0.318483] Failed to connect to bus: No such file or directory
[  +0.308231] Failed to connect to bus: No such file or directory
[  +0.322672] Failed to connect to bus: No such file or directory
[  +0.282972] Failed to connect to bus: No such file or directory
[  +0.392528] Failed to connect to bus: No such file or directory
[  +0.347961] Failed to connect to bus: No such file or directory
[  +0.360626] Failed to connect to bus: No such file or directory
[  +0.679681] Failed to connect to bus: No such file or directory
[  +0.288130] Failed to connect to bus: No such file or directory
[  +0.366445] Failed to connect to bus: No such file or directory
[  +0.353782] Failed to connect to bus: No such file or directory
[  +0.457538] Failed to connect to bus: No such file or directory
[  +4.380830] WSL (2 - init-systemd(Ubuntu)) ERROR: WaitForBootProcess:3497: /sbin/init failed to start within 10000ms
[  +0.059147] systemd-journald[112]: File /var/log/journal/99d44c81f43d448f9ede260e804062d9/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +2.072226] netlink: 'init': attribute type 4 has an invalid length.
[Jul 4 15:32] Exception: 
[  +0.000019] Operation canceled @p9io.cpp:258 (AcceptAsync)

[ +19.827041] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.131182] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004927] Failed to connect to bus: No such file or directory
[  +0.219351] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000325] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000775] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.406710] Failed to connect to bus: No such file or directory
[  +0.292877] Failed to connect to bus: No such file or directory
[  +0.267984] Failed to connect to bus: No such file or directory
[  +0.286003] Failed to connect to bus: No such file or directory
[  +0.277503] Failed to connect to bus: No such file or directory
[  +0.272786] Failed to connect to bus: No such file or directory
[  +0.267627] Failed to connect to bus: No such file or directory
[  +0.288786] Failed to connect to bus: No such file or directory
[  +1.780701] systemd-journald[80]: File /var/log/journal/99d44c81f43d448f9ede260e804062d9/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +5.049919] WSL (2 - init-systemd(Ubuntu)) ERROR: WaitForBootProcess:3497: /sbin/init failed to start within 10000ms
[  +6.136650] systemd-journald[80]: File /var/log/journal/99d44c81f43d448f9ede260e804062d9/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.
[Jul 4 15:41] hrtimer: interrupt took 12691523 ns


==> etcd [1c3e86269478] <==
{"level":"warn","ts":"2025-07-04T15:42:19.231621Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"296.386067ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-04T15:42:19.238083Z","caller":"traceutil/trace.go:171","msg":"trace[1900643130] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:860; }","duration":"302.916847ms","start":"2025-07-04T15:42:18.935125Z","end":"2025-07-04T15:42:19.238042Z","steps":["trace[1900643130] 'range keys from in-memory index tree'  (duration: 296.245739ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:19.238234Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-04T15:42:18.935089Z","time spent":"303.096515ms","remote":"127.0.0.1:35250","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-07-04T15:42:19.231927Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.398694ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/postgres-pvc\" limit:1 ","response":"range_response_count:1 size:1607"}
{"level":"info","ts":"2025-07-04T15:42:19.238379Z","caller":"traceutil/trace.go:171","msg":"trace[1991383296] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/postgres-pvc; range_end:; response_count:1; response_revision:860; }","duration":"199.94871ms","start":"2025-07-04T15:42:19.038393Z","end":"2025-07-04T15:42:19.238342Z","steps":["trace[1991383296] 'range keys from in-memory index tree'  (duration: 193.116797ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:19.449859Z","caller":"traceutil/trace.go:171","msg":"trace[173411480] linearizableReadLoop","detail":"{readStateIndex:924; appliedIndex:923; }","duration":"110.34934ms","start":"2025-07-04T15:42:19.339470Z","end":"2025-07-04T15:42:19.449819Z","steps":["trace[173411480] 'read index received'  (duration: 93.714209ms)","trace[173411480] 'applied index is now lower than readState.Index'  (duration: 16.630643ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-04T15:42:19.450441Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.937548ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-c83097a6-49a5-4918-9741-21cdce8866e8\" limit:1 ","response":"range_response_count:1 size:1214"}
{"level":"info","ts":"2025-07-04T15:42:19.450593Z","caller":"traceutil/trace.go:171","msg":"trace[310921748] range","detail":"{range_begin:/registry/persistentvolumes/pvc-c83097a6-49a5-4918-9741-21cdce8866e8; range_end:; response_count:1; response_revision:862; }","duration":"111.126789ms","start":"2025-07-04T15:42:19.339407Z","end":"2025-07-04T15:42:19.450533Z","steps":["trace[310921748] 'agreement among raft nodes before linearized reading'  (duration: 110.806964ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:19.451690Z","caller":"traceutil/trace.go:171","msg":"trace[1356076306] transaction","detail":"{read_only:false; response_revision:862; number_of_response:1; }","duration":"113.164336ms","start":"2025-07-04T15:42:19.338459Z","end":"2025-07-04T15:42:19.451623Z","steps":["trace[1356076306] 'process raft request'  (duration: 94.851891ms)","trace[1356076306] 'compare'  (duration: 16.188734ms)"],"step_count":2}
{"level":"info","ts":"2025-07-04T15:42:28.154994Z","caller":"traceutil/trace.go:171","msg":"trace[549155554] transaction","detail":"{read_only:false; response_revision:874; number_of_response:1; }","duration":"141.747406ms","start":"2025-07-04T15:42:28.013201Z","end":"2025-07-04T15:42:28.154948Z","steps":["trace[549155554] 'process raft request'  (duration: 141.311657ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:30.674328Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.551554ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038373245658927 > lease_revoke:<id:70cc97d6160d26cc>","response":"size:29"}
{"level":"warn","ts":"2025-07-04T15:42:32.396769Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"279.840834ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-07-04T15:42:32.396988Z","caller":"traceutil/trace.go:171","msg":"trace[1755588421] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:877; }","duration":"280.148315ms","start":"2025-07-04T15:42:32.116807Z","end":"2025-07-04T15:42:32.396956Z","steps":["trace[1755588421] 'count revisions from in-memory index tree'  (duration: 279.694276ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:32.397182Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"262.634833ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-07-04T15:42:32.397212Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"222.334581ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1108"}
{"level":"info","ts":"2025-07-04T15:42:32.397327Z","caller":"traceutil/trace.go:171","msg":"trace[348044403] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:877; }","duration":"262.792583ms","start":"2025-07-04T15:42:32.134502Z","end":"2025-07-04T15:42:32.397295Z","steps":["trace[348044403] 'range keys from in-memory index tree'  (duration: 262.559678ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:32.397345Z","caller":"traceutil/trace.go:171","msg":"trace[355508367] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:877; }","duration":"222.530844ms","start":"2025-07-04T15:42:32.174786Z","end":"2025-07-04T15:42:32.397317Z","steps":["trace[355508367] 'range keys from in-memory index tree'  (duration: 222.076695ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:32.397533Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"462.625016ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340\" limit:1 ","response":"range_response_count:1 size:721"}
{"level":"info","ts":"2025-07-04T15:42:32.397698Z","caller":"traceutil/trace.go:171","msg":"trace[978248207] range","detail":"{range_begin:/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340; range_end:; response_count:1; response_revision:877; }","duration":"462.782907ms","start":"2025-07-04T15:42:31.934818Z","end":"2025-07-04T15:42:32.397601Z","steps":["trace[978248207] 'range keys from in-memory index tree'  (duration: 462.393828ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:32.397784Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-04T15:42:31.934770Z","time spent":"462.985921ms","remote":"127.0.0.1:35352","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":745,"request content":"key:\"/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340\" limit:1 "}
{"level":"info","ts":"2025-07-04T15:42:32.945511Z","caller":"traceutil/trace.go:171","msg":"trace[1200240795] transaction","detail":"{read_only:false; response_revision:878; number_of_response:1; }","duration":"543.862379ms","start":"2025-07-04T15:42:32.401603Z","end":"2025-07-04T15:42:32.945465Z","steps":["trace[1200240795] 'process raft request'  (duration: 543.446006ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:32.945892Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-04T15:42:32.401561Z","time spent":"544.179775ms","remote":"127.0.0.1:35352","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":706,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340\" mod_revision:861 > success:<request_put:<key:\"/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340\" value_size:623 lease:8128038373245658659 >> failure:<request_range:<key:\"/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340\" > >"}
{"level":"info","ts":"2025-07-04T15:42:33.011318Z","caller":"traceutil/trace.go:171","msg":"trace[1425540803] linearizableReadLoop","detail":"{readStateIndex:944; appliedIndex:943; }","duration":"344.778487ms","start":"2025-07-04T15:42:32.666496Z","end":"2025-07-04T15:42:33.011275Z","steps":["trace[1425540803] 'read index received'  (duration: 278.870098ms)","trace[1425540803] 'applied index is now lower than readState.Index'  (duration: 65.906976ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-04T15:42:33.011611Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"345.114056ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-04T15:42:33.011598Z","caller":"traceutil/trace.go:171","msg":"trace[1317791880] transaction","detail":"{read_only:false; response_revision:879; number_of_response:1; }","duration":"595.873321ms","start":"2025-07-04T15:42:32.415671Z","end":"2025-07-04T15:42:33.011544Z","steps":["trace[1317791880] 'process raft request'  (duration: 587.187133ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:33.011680Z","caller":"traceutil/trace.go:171","msg":"trace[641175661] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:879; }","duration":"345.264622ms","start":"2025-07-04T15:42:32.666395Z","end":"2025-07-04T15:42:33.011659Z","steps":["trace[641175661] 'agreement among raft nodes before linearized reading'  (duration: 345.131449ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:33.011741Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-04T15:42:32.666348Z","time spent":"345.377592ms","remote":"127.0.0.1:35250","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-07-04T15:42:33.011844Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-04T15:42:32.415629Z","time spent":"596.077642ms","remote":"127.0.0.1:35418","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:876 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-07-04T15:42:33.528451Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"393.52782ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-04T15:42:33.528589Z","caller":"traceutil/trace.go:171","msg":"trace[31102171] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:879; }","duration":"393.685755ms","start":"2025-07-04T15:42:33.134872Z","end":"2025-07-04T15:42:33.528558Z","steps":["trace[31102171] 'range keys from in-memory index tree'  (duration: 393.447163ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:33.528574Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.321911ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-04T15:42:33.528701Z","caller":"traceutil/trace.go:171","msg":"trace[182655272] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:879; }","duration":"194.472547ms","start":"2025-07-04T15:42:33.334195Z","end":"2025-07-04T15:42:33.528667Z","steps":["trace[182655272] 'range keys from in-memory index tree'  (duration: 194.232884ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:43.289128Z","caller":"traceutil/trace.go:171","msg":"trace[1660551286] transaction","detail":"{read_only:false; response_revision:893; number_of_response:1; }","duration":"112.450944ms","start":"2025-07-04T15:42:43.176635Z","end":"2025-07-04T15:42:43.289086Z","steps":["trace[1660551286] 'process raft request'  (duration: 107.644049ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:42:46.267493Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.737849ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-07-04T15:42:46.269448Z","caller":"traceutil/trace.go:171","msg":"trace[1378633739] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:898; }","duration":"102.217873ms","start":"2025-07-04T15:42:46.166657Z","end":"2025-07-04T15:42:46.268875Z","steps":["trace[1378633739] 'range keys from in-memory index tree'  (duration: 100.601684ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:46.399496Z","caller":"traceutil/trace.go:171","msg":"trace[880218336] linearizableReadLoop","detail":"{readStateIndex:967; appliedIndex:966; }","duration":"106.304515ms","start":"2025-07-04T15:42:46.293153Z","end":"2025-07-04T15:42:46.399457Z","steps":["trace[880218336] 'read index received'  (duration: 105.968625ms)","trace[880218336] 'applied index is now lower than readState.Index'  (duration: 334.893Âµs)"],"step_count":2}
{"level":"warn","ts":"2025-07-04T15:42:46.400468Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.316953ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2025-07-04T15:42:46.400829Z","caller":"traceutil/trace.go:171","msg":"trace[476082214] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:899; }","duration":"107.771867ms","start":"2025-07-04T15:42:46.292997Z","end":"2025-07-04T15:42:46.400769Z","steps":["trace[476082214] 'agreement among raft nodes before linearized reading'  (duration: 106.683079ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:46.475121Z","caller":"traceutil/trace.go:171","msg":"trace[1633298832] transaction","detail":"{read_only:false; response_revision:899; number_of_response:1; }","duration":"183.809757ms","start":"2025-07-04T15:42:46.291247Z","end":"2025-07-04T15:42:46.475057Z","steps":["trace[1633298832] 'process raft request'  (duration: 107.99122ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:42:56.086535Z","caller":"traceutil/trace.go:171","msg":"trace[1756013621] transaction","detail":"{read_only:false; response_revision:909; number_of_response:1; }","duration":"116.148512ms","start":"2025-07-04T15:42:55.970333Z","end":"2025-07-04T15:42:56.086481Z","steps":["trace[1756013621] 'process raft request'  (duration: 95.986694ms)","trace[1756013621] 'compare'  (duration: 19.934323ms)"],"step_count":2}
{"level":"info","ts":"2025-07-04T15:42:56.695269Z","caller":"traceutil/trace.go:171","msg":"trace[1749444420] transaction","detail":"{read_only:false; response_revision:912; number_of_response:1; }","duration":"119.813144ms","start":"2025-07-04T15:42:56.575411Z","end":"2025-07-04T15:42:56.695219Z","steps":["trace[1749444420] 'process raft request'  (duration: 62.547937ms)","trace[1749444420] 'compare'  (duration: 56.890115ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-04T15:42:58.698418Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.631931ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340\" limit:1 ","response":"range_response_count:1 size:721"}
{"level":"info","ts":"2025-07-04T15:42:58.698569Z","caller":"traceutil/trace.go:171","msg":"trace[745021828] range","detail":"{range_begin:/registry/events/default/frontend-57f7fbf4-24n2c.184f169edbdc2340; range_end:; response_count:1; response_revision:915; }","duration":"131.856916ms","start":"2025-07-04T15:42:58.566677Z","end":"2025-07-04T15:42:58.698534Z","steps":["trace[745021828] 'range keys from in-memory index tree'  (duration: 131.448092ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:43:03.461661Z","caller":"traceutil/trace.go:171","msg":"trace[808073110] transaction","detail":"{read_only:false; response_revision:919; number_of_response:1; }","duration":"146.321044ms","start":"2025-07-04T15:43:03.315299Z","end":"2025-07-04T15:43:03.461620Z","steps":["trace[808073110] 'process raft request'  (duration: 145.839334ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:43:05.703603Z","caller":"traceutil/trace.go:171","msg":"trace[604108600] transaction","detail":"{read_only:false; response_revision:921; number_of_response:1; }","duration":"118.441199ms","start":"2025-07-04T15:43:05.585113Z","end":"2025-07-04T15:43:05.703555Z","steps":["trace[604108600] 'process raft request'  (duration: 117.883271ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:43:17.264463Z","caller":"traceutil/trace.go:171","msg":"trace[1082963383] transaction","detail":"{read_only:false; response_revision:933; number_of_response:1; }","duration":"113.681673ms","start":"2025-07-04T15:43:17.150734Z","end":"2025-07-04T15:43:17.264415Z","steps":["trace[1082963383] 'process raft request'  (duration: 95.423894ms)","trace[1082963383] 'compare'  (duration: 18.086557ms)"],"step_count":2}
{"level":"info","ts":"2025-07-04T15:43:19.250437Z","caller":"traceutil/trace.go:171","msg":"trace[1441145818] linearizableReadLoop","detail":"{readStateIndex:1009; appliedIndex:1008; }","duration":"201.101463ms","start":"2025-07-04T15:43:19.049281Z","end":"2025-07-04T15:43:19.250383Z","steps":["trace[1441145818] 'read index received'  (duration: 200.609275ms)","trace[1441145818] 'applied index is now lower than readState.Index'  (duration: 491.076Âµs)"],"step_count":2}
{"level":"warn","ts":"2025-07-04T15:43:19.250886Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.552019ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-04T15:43:19.250991Z","caller":"traceutil/trace.go:171","msg":"trace[273782685] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:934; }","duration":"201.732909ms","start":"2025-07-04T15:43:19.049224Z","end":"2025-07-04T15:43:19.250957Z","steps":["trace[273782685] 'agreement among raft nodes before linearized reading'  (duration: 201.371023ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:43:19.251660Z","caller":"traceutil/trace.go:171","msg":"trace[283901382] transaction","detail":"{read_only:false; response_revision:934; number_of_response:1; }","duration":"600.746672ms","start":"2025-07-04T15:43:18.650888Z","end":"2025-07-04T15:43:19.251634Z","steps":["trace[283901382] 'process raft request'  (duration: 599.127881ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:43:19.251824Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-04T15:43:18.650840Z","time spent":"600.87579ms","remote":"127.0.0.1:35530","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:925 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-07-04T15:43:19.645676Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"293.997569ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-07-04T15:43:19.646338Z","caller":"traceutil/trace.go:171","msg":"trace[83336247] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:934; }","duration":"294.661305ms","start":"2025-07-04T15:43:19.351564Z","end":"2025-07-04T15:43:19.646225Z","steps":["trace[83336247] 'range keys from in-memory index tree'  (duration: 293.829389ms)"],"step_count":1}
{"level":"info","ts":"2025-07-04T15:43:24.247068Z","caller":"traceutil/trace.go:171","msg":"trace[1435009922] transaction","detail":"{read_only:false; response_revision:937; number_of_response:1; }","duration":"191.825341ms","start":"2025-07-04T15:43:24.055076Z","end":"2025-07-04T15:43:24.246901Z","steps":["trace[1435009922] 'process raft request'  (duration: 190.707639ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:43:24.446137Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.754513ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-04T15:43:24.446309Z","caller":"traceutil/trace.go:171","msg":"trace[664116490] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:937; }","duration":"184.942861ms","start":"2025-07-04T15:43:24.261330Z","end":"2025-07-04T15:43:24.446273Z","steps":["trace[664116490] 'range keys from in-memory index tree'  (duration: 184.630942ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:43:31.945853Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.25055ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038373245659269 > lease_revoke:<id:70cc97d6160d2836>","response":"size:29"}
{"level":"info","ts":"2025-07-04T15:43:32.823690Z","caller":"traceutil/trace.go:171","msg":"trace[163636190] transaction","detail":"{read_only:false; response_revision:944; number_of_response:1; }","duration":"149.970841ms","start":"2025-07-04T15:43:32.673659Z","end":"2025-07-04T15:43:32.823630Z","steps":["trace[163636190] 'process raft request'  (duration: 149.214655ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-04T15:43:33.103193Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"220.351958ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:4576"}
{"level":"info","ts":"2025-07-04T15:43:33.103399Z","caller":"traceutil/trace.go:171","msg":"trace[1166533337] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:944; }","duration":"220.635618ms","start":"2025-07-04T15:43:32.882728Z","end":"2025-07-04T15:43:33.103363Z","steps":["trace[1166533337] 'range keys from in-memory index tree'  (duration: 219.922476ms)"],"step_count":1}


==> kernel <==
 15:43:34 up 13 min,  0 users,  load average: 2.62, 2.11, 1.29
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a00d2ca4abe8] <==
I0704 15:37:50.926581       1 controller.go:90] Starting OpenAPI V3 controller
I0704 15:37:50.927047       1 naming_controller.go:299] Starting NamingConditionController
I0704 15:37:50.927498       1 establishing_controller.go:81] Starting EstablishingController
I0704 15:37:50.928185       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0704 15:37:50.928478       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0704 15:37:50.928713       1 crd_finalizer.go:269] Starting CRDFinalizer
I0704 15:37:51.215288       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0704 15:37:51.215400       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0704 15:37:51.423436       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0704 15:37:51.428592       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0704 15:37:51.619182       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0704 15:37:51.619351       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0704 15:37:51.621020       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0704 15:37:51.715022       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0704 15:37:51.715138       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0704 15:37:51.715470       1 policy_source.go:240] refreshing policies
I0704 15:37:51.715627       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0704 15:37:51.715710       1 aggregator.go:171] initial CRD sync complete...
I0704 15:37:51.715955       1 controller.go:667] quota admission added evaluator for: namespaces
I0704 15:37:51.717020       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0704 15:37:51.717083       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0704 15:37:51.718313       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0704 15:37:51.718864       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0704 15:37:51.718972       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0704 15:37:51.719047       1 cache.go:39] Caches are synced for LocalAvailability controller
I0704 15:37:51.719151       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0704 15:37:51.719610       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0704 15:37:51.721866       1 autoregister_controller.go:144] Starting autoregister controller
I0704 15:37:51.721999       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0704 15:37:51.722063       1 cache.go:39] Caches are synced for autoregister controller
E0704 15:37:51.824339       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0704 15:37:51.828455       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0704 15:37:51.828452       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0704 15:37:51.917018       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0704 15:37:51.923845       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0704 15:37:52.117004       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0704 15:37:52.126271       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0704 15:37:52.382782       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0704 15:37:52.382889       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0704 15:37:59.569786       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0704 15:37:59.851970       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0704 15:38:00.415890       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0704 15:38:00.483387       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0704 15:38:00.515186       1 controller.go:667] quota admission added evaluator for: endpoints
I0704 15:38:00.536327       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0704 15:38:01.432011       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0704 15:38:03.613004       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0704 15:38:03.859820       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0704 15:38:04.028379       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0704 15:38:13.812304       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0704 15:38:13.918203       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0704 15:38:14.026447       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0704 15:38:14.219819       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0704 15:38:14.717684       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0704 15:40:47.807184       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0704 15:40:47.814440       1 alloc.go:328] "allocated clusterIPs" service="default/frontend" clusterIPs={"IPv4":"10.96.139.55"}
I0704 15:40:48.494426       1 alloc.go:328] "allocated clusterIPs" service="default/mongo-db" clusterIPs={"IPv4":"10.102.189.197"}
I0704 15:40:48.498679       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0704 15:40:49.816665       1 alloc.go:328] "allocated clusterIPs" service="default/postgres" clusterIPs={"IPv4":"10.107.160.72"}
I0704 15:40:49.832661       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [cda789bec201] <==
I0704 15:38:12.237620       1 shared_informer.go:350] "Waiting for caches to sync" controller="bootstrap_signer"
I0704 15:38:12.526463       1 controllermanager.go:778] "Started controller" controller="ttl-after-finished-controller"
I0704 15:38:12.526695       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="kube-apiserver-serving-clustertrustbundle-publisher-controller" requiredFeatureGates=["ClusterTrustBundle"]
I0704 15:38:12.527214       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I0704 15:38:12.527345       1 shared_informer.go:350] "Waiting for caches to sync" controller="TTL after finished"
I0704 15:38:12.528870       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="resourceclaim-controller" requiredFeatureGates=["DynamicResourceAllocation"]
I0704 15:38:12.633990       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0704 15:38:12.812441       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0704 15:38:12.813115       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0704 15:38:12.822626       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0704 15:38:12.942096       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0704 15:38:13.012663       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0704 15:38:13.014801       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0704 15:38:13.020113       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0704 15:38:13.020487       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0704 15:38:13.020544       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0704 15:38:13.020666       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0704 15:38:13.023114       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0704 15:38:13.023352       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0704 15:38:13.023389       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0704 15:38:13.028342       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0704 15:38:13.031841       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0704 15:38:13.044156       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0704 15:38:13.113893       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0704 15:38:13.114150       1 shared_informer.go:357] "Caches are synced" controller="node"
I0704 15:38:13.115095       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0704 15:38:13.115289       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0704 15:38:13.115377       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0704 15:38:13.115400       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0704 15:38:13.133134       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0704 15:38:13.133549       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0704 15:38:13.138709       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0704 15:38:13.212306       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0704 15:38:13.212596       1 shared_informer.go:357] "Caches are synced" controller="job"
I0704 15:38:13.214659       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0704 15:38:13.222147       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0704 15:38:13.223093       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0704 15:38:13.223179       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0704 15:38:13.223246       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0704 15:38:13.223301       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0704 15:38:13.223355       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0704 15:38:13.224152       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0704 15:38:13.224868       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0704 15:38:13.224910       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0704 15:38:13.225141       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0704 15:38:13.225210       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0704 15:38:13.225357       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0704 15:38:13.225519       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0704 15:38:13.225597       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0704 15:38:13.228424       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0704 15:38:13.228481       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0704 15:38:13.228796       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0704 15:38:13.229787       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0704 15:38:13.319996       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0704 15:38:13.412140       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0704 15:38:13.413235       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0704 15:38:13.717233       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0704 15:38:13.717291       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0704 15:38:13.717309       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0704 15:38:13.720388       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-proxy [5c1db6a0cd4a] <==
I0704 15:38:24.532358       1 server_linux.go:63] "Using iptables proxy"
I0704 15:38:25.427464       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0704 15:38:25.427826       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0704 15:38:26.117110       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0704 15:38:26.117332       1 server_linux.go:145] "Using iptables Proxier"
I0704 15:38:26.225164       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0704 15:38:26.318673       1 server.go:516] "Version info" version="v1.33.1"
I0704 15:38:26.318770       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0704 15:38:26.418017       1 config.go:329] "Starting node config controller"
I0704 15:38:26.418118       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0704 15:38:26.430014       1 config.go:105] "Starting endpoint slice config controller"
I0704 15:38:26.430135       1 config.go:440] "Starting serviceCIDR config controller"
I0704 15:38:26.430171       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0704 15:38:26.430097       1 config.go:199] "Starting service config controller"
I0704 15:38:26.430201       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0704 15:38:26.430139       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0704 15:38:26.518320       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0704 15:38:26.533874       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0704 15:38:26.533902       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0704 15:38:26.630708       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [b4ad6bf0b171] <==
I0704 15:37:44.355087       1 serving.go:386] Generated self-signed cert in-memory
W0704 15:37:55.548810       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0704 15:37:55.548970       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0704 15:37:55.549008       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0704 15:37:55.549032       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0704 15:37:55.664951       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0704 15:37:55.665113       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0704 15:37:55.715074       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0704 15:37:55.715196       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0704 15:37:55.715199       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0704 15:37:55.715316       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0704 15:37:55.725251       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0704 15:37:55.727970       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0704 15:37:55.734030       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0704 15:37:55.734308       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0704 15:37:55.734509       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0704 15:37:55.734705       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0704 15:37:55.734872       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0704 15:37:55.739244       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0704 15:37:55.739437       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0704 15:37:55.750876       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0704 15:37:55.751321       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0704 15:37:55.751543       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0704 15:37:55.751931       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0704 15:37:55.752081       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0704 15:37:55.752597       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0704 15:37:55.815315       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0704 15:37:56.559876       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0704 15:37:56.587043       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0704 15:37:56.737278       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0704 15:37:56.748824       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0704 15:37:56.820962       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0704 15:37:56.821009       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0704 15:37:56.847996       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0704 15:37:56.854573       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0704 15:37:56.855081       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0704 15:37:56.919456       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0704 15:37:56.950141       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0704 15:37:57.016123       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0704 15:37:57.073399       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0704 15:37:57.073440       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0704 15:37:57.234490       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0704 15:37:57.330096       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0704 15:37:59.654888       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0704 15:38:07.019394       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0704 15:40:48.117286       1 schedule_one.go:160] "Error selecting node for pod" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mongo-pvc\": could not find v1.PersistentVolumeClaim \"default/mongo-pvc\"" pod="default/mongo-db-54cb6f854f-c8jr5"
E0704 15:40:48.117394       1 schedule_one.go:1046] "Error scheduling pod; retrying" err="running PreFilter plugin \"VolumeBinding\": error getting PVC \"default/mongo-pvc\": could not find v1.PersistentVolumeClaim \"default/mongo-pvc\"" pod="default/mongo-db-54cb6f854f-c8jr5"


==> kubelet <==
Jul 04 15:42:02 minikube kubelet[3273]: E0704 15:42:02.278675    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:02 minikube kubelet[3273]: E0704 15:42:02.280274    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:42:05 minikube kubelet[3273]: E0704 15:42:05.277331    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:42:05 minikube kubelet[3273]: E0704 15:42:05.280820    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"
Jul 04 15:42:12 minikube kubelet[3273]: E0704 15:42:12.867592    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:monitoring-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f96bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod monitoring-service-9d4457dc-bnz96_default(0873446c-2b2b-4939-ac83-12de62cf0b27): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:12 minikube kubelet[3273]: E0704 15:42:12.869129    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"monitoring-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase\"" pod="default/monitoring-service-9d4457dc-bnz96" podUID="0873446c-2b2b-4939-ac83-12de62cf0b27"
Jul 04 15:42:13 minikube kubelet[3273]: E0704 15:42:13.867391    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:auth-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_PASSWORD,Value:password,ValueFrom:nil,},EnvVar{Name:POSTGRES_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:POSTGRES_HOST,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn65c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-service-6cf6d597f4-hck22_default(1e5701d2-4373-4d63-9431-b64bdd615a5c): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:13 minikube kubelet[3273]: E0704 15:42:13.868807    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase\"" pod="default/auth-service-6cf6d597f4-hck22" podUID="1e5701d2-4373-4d63-9431-b64bdd615a5c"
Jul 04 15:42:14 minikube kubelet[3273]: E0704 15:42:14.867865    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:14 minikube kubelet[3273]: E0704 15:42:14.869320    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:42:18 minikube kubelet[3273]: E0704 15:42:17.939270    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:42:18 minikube kubelet[3273]: E0704 15:42:18.032566    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"
Jul 04 15:42:24 minikube kubelet[3273]: E0704 15:42:24.867806    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:monitoring-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f96bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod monitoring-service-9d4457dc-bnz96_default(0873446c-2b2b-4939-ac83-12de62cf0b27): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:24 minikube kubelet[3273]: E0704 15:42:24.869500    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"monitoring-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase\"" pod="default/monitoring-service-9d4457dc-bnz96" podUID="0873446c-2b2b-4939-ac83-12de62cf0b27"
Jul 04 15:42:27 minikube kubelet[3273]: E0704 15:42:27.867794    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:auth-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_PASSWORD,Value:password,ValueFrom:nil,},EnvVar{Name:POSTGRES_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:POSTGRES_HOST,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn65c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-service-6cf6d597f4-hck22_default(1e5701d2-4373-4d63-9431-b64bdd615a5c): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:27 minikube kubelet[3273]: E0704 15:42:27.868297    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:27 minikube kubelet[3273]: E0704 15:42:27.869223    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase\"" pod="default/auth-service-6cf6d597f4-hck22" podUID="1e5701d2-4373-4d63-9431-b64bdd615a5c"
Jul 04 15:42:27 minikube kubelet[3273]: E0704 15:42:27.869781    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:42:31 minikube kubelet[3273]: E0704 15:42:31.867273    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:42:31 minikube kubelet[3273]: E0704 15:42:31.868873    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"
Jul 04 15:42:35 minikube kubelet[3273]: E0704 15:42:35.867956    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:monitoring-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f96bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod monitoring-service-9d4457dc-bnz96_default(0873446c-2b2b-4939-ac83-12de62cf0b27): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:35 minikube kubelet[3273]: E0704 15:42:35.869438    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"monitoring-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase\"" pod="default/monitoring-service-9d4457dc-bnz96" podUID="0873446c-2b2b-4939-ac83-12de62cf0b27"
Jul 04 15:42:42 minikube kubelet[3273]: E0704 15:42:42.491235    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:auth-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_PASSWORD,Value:password,ValueFrom:nil,},EnvVar{Name:POSTGRES_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:POSTGRES_HOST,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn65c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-service-6cf6d597f4-hck22_default(1e5701d2-4373-4d63-9431-b64bdd615a5c): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:42 minikube kubelet[3273]: E0704 15:42:42.492556    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:42 minikube kubelet[3273]: E0704 15:42:42.492983    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase\"" pod="default/auth-service-6cf6d597f4-hck22" podUID="1e5701d2-4373-4d63-9431-b64bdd615a5c"
Jul 04 15:42:42 minikube kubelet[3273]: E0704 15:42:42.494277    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:42:43 minikube kubelet[3273]: E0704 15:42:43.402349    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:42:43 minikube kubelet[3273]: E0704 15:42:43.467559    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"
Jul 04 15:42:51 minikube kubelet[3273]: E0704 15:42:51.481126    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:monitoring-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f96bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod monitoring-service-9d4457dc-bnz96_default(0873446c-2b2b-4939-ac83-12de62cf0b27): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:51 minikube kubelet[3273]: E0704 15:42:51.483976    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"monitoring-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase\"" pod="default/monitoring-service-9d4457dc-bnz96" podUID="0873446c-2b2b-4939-ac83-12de62cf0b27"
Jul 04 15:42:54 minikube kubelet[3273]: E0704 15:42:54.403428    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:54 minikube kubelet[3273]: E0704 15:42:54.405150    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:42:56 minikube kubelet[3273]: E0704 15:42:56.414317    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:auth-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_PASSWORD,Value:password,ValueFrom:nil,},EnvVar{Name:POSTGRES_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:POSTGRES_HOST,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn65c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-service-6cf6d597f4-hck22_default(1e5701d2-4373-4d63-9431-b64bdd615a5c): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase" logger="UnhandledError"
Jul 04 15:42:56 minikube kubelet[3273]: E0704 15:42:56.416019    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase\"" pod="default/auth-service-6cf6d597f4-hck22" podUID="1e5701d2-4373-4d63-9431-b64bdd615a5c"
Jul 04 15:42:58 minikube kubelet[3273]: E0704 15:42:58.406682    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:42:58 minikube kubelet[3273]: E0704 15:42:58.408049    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"
Jul 04 15:43:05 minikube kubelet[3273]: E0704 15:43:05.403550    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:monitoring-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f96bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod monitoring-service-9d4457dc-bnz96_default(0873446c-2b2b-4939-ac83-12de62cf0b27): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:05 minikube kubelet[3273]: E0704 15:43:05.411657    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"monitoring-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase\"" pod="default/monitoring-service-9d4457dc-bnz96" podUID="0873446c-2b2b-4939-ac83-12de62cf0b27"
Jul 04 15:43:07 minikube kubelet[3273]: E0704 15:43:07.402559    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:auth-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_PASSWORD,Value:password,ValueFrom:nil,},EnvVar{Name:POSTGRES_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:POSTGRES_HOST,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn65c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-service-6cf6d597f4-hck22_default(1e5701d2-4373-4d63-9431-b64bdd615a5c): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:07 minikube kubelet[3273]: E0704 15:43:07.404130    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase\"" pod="default/auth-service-6cf6d597f4-hck22" podUID="1e5701d2-4373-4d63-9431-b64bdd615a5c"
Jul 04 15:43:09 minikube kubelet[3273]: E0704 15:43:09.982563    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:09 minikube kubelet[3273]: E0704 15:43:09.984893    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:43:10 minikube kubelet[3273]: E0704 15:43:10.985873    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:43:10 minikube kubelet[3273]: E0704 15:43:10.987245    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"
Jul 04 15:43:16 minikube kubelet[3273]: E0704 15:43:16.988337    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:monitoring-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f96bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod monitoring-service-9d4457dc-bnz96_default(0873446c-2b2b-4939-ac83-12de62cf0b27): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:17 minikube kubelet[3273]: E0704 15:43:16.993489    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"monitoring-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase\"" pod="default/monitoring-service-9d4457dc-bnz96" podUID="0873446c-2b2b-4939-ac83-12de62cf0b27"
Jul 04 15:43:21 minikube kubelet[3273]: E0704 15:43:20.990967    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:21 minikube kubelet[3273]: E0704 15:43:21.045572    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:43:21 minikube kubelet[3273]: E0704 15:43:21.985764    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:auth-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_PASSWORD,Value:password,ValueFrom:nil,},EnvVar{Name:POSTGRES_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:POSTGRES_HOST,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn65c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-service-6cf6d597f4-hck22_default(1e5701d2-4373-4d63-9431-b64bdd615a5c): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:21 minikube kubelet[3273]: E0704 15:43:21.987467    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase\"" pod="default/auth-service-6cf6d597f4-hck22" podUID="1e5701d2-4373-4d63-9431-b64bdd615a5c"
Jul 04 15:43:23 minikube kubelet[3273]: E0704 15:43:23.087570    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:43:23 minikube kubelet[3273]: E0704 15:43:23.145834    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"
Jul 04 15:43:31 minikube kubelet[3273]: E0704 15:43:31.033840    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:monitoring-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f96bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod monitoring-service-9d4457dc-bnz96_default(0873446c-2b2b-4939-ac83-12de62cf0b27): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:31 minikube kubelet[3273]: E0704 15:43:31.035416    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"monitoring-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-monitoring_service) must be lowercase\"" pod="default/monitoring-service-9d4457dc-bnz96" podUID="0873446c-2b2b-4939-ac83-12de62cf0b27"
Jul 04 15:43:32 minikube kubelet[3273]: E0704 15:43:32.982136    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:travel-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-db:27017/travel_planner,ValueFrom:nil,},EnvVar{Name:MONGODB_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_INTERNAL_URL,Value:http://auth-service:80/,ValueFrom:nil,},EnvVar{Name:AUTH_SERVICE_EXTERNAL_URL,Value:http://localhost:8001/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zhg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:travel-service-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod travel-service-697db68659-n5t4r_default(01b1bdc8-cc3c-48ab-b6c9-971d59ccc487): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:32 minikube kubelet[3273]: E0704 15:43:32.984115    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"travel-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-travel_service) must be lowercase\"" pod="default/travel-service-697db68659-n5t4r" podUID="01b1bdc8-cc3c-48ab-b6c9-971d59ccc487"
Jul 04 15:43:33 minikube kubelet[3273]: E0704 15:43:33.986383    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:auth-service,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_PASSWORD,Value:password,ValueFrom:nil,},EnvVar{Name:POSTGRES_DB,Value:travel_planner,ValueFrom:nil,},EnvVar{Name:POSTGRES_HOST,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nn65c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod auth-service-6cf6d597f4-hck22_default(1e5701d2-4373-4d63-9431-b64bdd615a5c): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase" logger="UnhandledError"
Jul 04 15:43:34 minikube kubelet[3273]: E0704 15:43:33.988759    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"auth-service\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-auth_service) must be lowercase\"" pod="default/auth-service-6cf6d597f4-hck22" podUID="1e5701d2-4373-4d63-9431-b64bdd615a5c"
Jul 04 15:43:37 minikube kubelet[3273]: E0704 15:43:37.072662    3273 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:frontend,Image:ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:TRAVEL_SERVICE_URL,Value:http://travel-service:80/,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-57f7fbf4-24n2c_default(ad03d662-1bd7-4e27-99df-9ea5770b1ad9): InvalidImageName: Failed to apply default image tag \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": couldn't parse image name \"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase" logger="UnhandledError"
Jul 04 15:43:37 minikube kubelet[3273]: E0704 15:43:37.077296    3273 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with InvalidImageName: \"Failed to apply default image tag \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": couldn't parse image name \\\"ghcr.io/${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend:latest\\\": invalid reference format: repository name (${GITHUB_REPOSITORY_OWNER}/travel-planner-frontend) must be lowercase\"" pod="default/frontend-57f7fbf4-24n2c" podUID="ad03d662-1bd7-4e27-99df-9ea5770b1ad9"


==> storage-provisioner [ac44aead8e6d] <==
W0704 15:42:32.400099       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:33.014690       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:35.039593       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:35.155177       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:37.699611       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:37.745159       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:39.768862       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:39.799405       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:41.868479       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:41.976842       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:44.002199       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:44.078192       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:46.272869       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:46.494741       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:48.506026       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:48.567765       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:50.615802       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:50.672076       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:52.686835       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:52.768379       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:54.788816       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:54.970791       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:56.984931       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:57.070393       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:59.080240       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:42:59.177138       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:01.194524       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:01.268056       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:03.277827       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:03.466802       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:05.577973       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:05.706814       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:08.354941       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:08.454982       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:10.482100       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:10.531428       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:12.539768       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:12.834188       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:14.890888       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:14.971664       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:16.984504       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:17.147471       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:19.650164       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:19.750182       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:21.850389       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:21.949278       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:23.958949       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:24.250003       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:26.266154       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:26.353825       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:28.375943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:28.585966       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:30.594092       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:30.639779       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:32.659919       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:32.827652       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:34.867743       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:34.947238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:37.005357       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0704 15:43:37.071535       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

